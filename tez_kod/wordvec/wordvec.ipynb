{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mng Kargo Nerede Olduğu Bilinmeyen Sipariş--> 16.03.2020 de yedpa şubeden çıkıp hala araçta görünen ilk kargom 17.03.2020de Adana aktarmaya gelip orada hareket görmeyen ikinci kargom 18.03.2020de İstanbul aktarmada takılan üçüncü kargom sorduğumda yoğunuz bu işi beceremiyorsanız yapmayın kardeşim tam üç kargoda hafta başından beri kargo bekliyorum yoğunluk corona bahane. Kabullenin ucuz fiyat politikası uygulayarak vatandaşı mağdur etmeyin. Hakkını verin işin insanlar sizin sorumsuzluğunun cezasını çekmek zorunda değil şubedeki bayanın lafına da bakar mısınız kargonuz kayıtsız gelebilir her an nereden çıkacağı belli olmaz.şu tüketici heyetin kargo firmalarına da yaptırımları olsun artık bıktık her zaman mağduriyet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mng', 'Kargo', 'Nerede', 'Olduğu', 'Bilinmeyen', 'Sipariş', '-->', '16', '.', '03', '.', '2020', 'de', 'yedpa', 'şubeden', 'çıkıp', 'hala', 'araçta', 'görünen', 'ilk', 'kargom', '17', '.', '03', '.', '2020de', 'Adana', 'aktarmaya', 'gelip', 'orada', 'hareket', 'görmeyen', 'ikinci', 'kargom', '18', '.', '03', '.', '2020de', 'İstanbul', 'aktarmada', 'takılan', 'üçüncü', 'kargom', 'sorduğumda', 'yoğunuz', 'bu', 'işi', 'beceremiyorsanız', 'yapmayın', 'kardeşim', 'tam', 'üç', 'kargoda', 'hafta', 'başından', 'beri', 'kargo', 'bekliyorum', 'yoğunluk', 'corona', 'bahane', '.', 'Kabullenin', 'ucuz', 'fiyat', 'politikası', 'uygulayarak', 'vatandaşı', 'mağdur', 'etmeyin', '.', 'Hakkını', 'verin', 'işin', 'insanlar', 'sizin', 'sorumsuzluğunun', 'cezasını', 'çekmek', 'zorunda', 'değil', 'şubedeki', 'bayanın', 'lafına', 'da', 'bakar', 'mısınız', 'kargonuz', 'kayıtsız', 'gelebilir', 'her', 'an', 'nereden', 'çıkacağı', 'belli', 'olmaz', '.', 'şu', 'tüketici', 'heyetin', 'kargo', 'firmalarına', 'da', 'yaptırımları', 'olsun', 'artık', 'bıktık', 'her', 'zaman', 'mağduriyet', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return [token.lower() for token in tokenizer.tokenize(sentence)]\n",
    "\n",
    "data_tok = [tokenize_sentence(sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aras kargo teslim etmiyor !--> 734332062246 no . lu gönderimde tam 4 gündür şubede tutulduğunu görüyorum , kargoyu alıcı kabul etmiyor diye bilgilendirme düşüyorlar online bilgilendirmede , 4 gündür eşim kargoyu bekliyor kesinlikle alıcı kabul etmiyor diye bir durum söz konusu değil , insanları işin gücünden alıkoyuyorsunuz yapacağınız basitçe bir kargoyu götürüp teslim etmek , 100 yıllık çözülemeyen bir matematik problemini çözmenizi istemiyoruz veya dünyanın en güzel kokan parfümü veya corona virüsüne antidoz geliştirmenizi de istemiyoruz , bir paketi a noktasından b noktasına taşıdığınızda ve alıcıya teslim ettiğinizde mutlu oluyoruz fakat siz de işinizi yapmış oluyorsunuz , basit bir süreç olduğuna inanıyorum . kargoyu getirmeden alıcı kabul etmiyor diyerek şirketin kalitesini dibe vurduruyorsunuz .', \"mng kargo dağıtım yapmıyor !--> 11 . 02 . 2020 tarihinde akçay ( edremit ) şubesine gelen kargomu 15 . 02 . 2020 tarihine kadar alamadım . kargoyu aradığımda kuryelerinin hastalandığı için dağıtamadıklarını belirttiler . geçici bir çözüm neden bulamadıklarını sorduğumda şirketin buna izin vermediğini belirttiler . kuryenin hastalanması şirketin sorunu olması gerekirken , hizmetin bedelini ödemiş müşterinin sorunu oluyor . i̇simine baktığınızda kocaman şirketlerin nasıl yönetildiklerine inanamıyorsunuz . duyan zanneder ki korona virüsü çin ' de değil türkiye ' de salgın olmuş .\"]\n"
     ]
    }
   ],
   "source": [
    "print([' '.join(row) for row in data_tok[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "model = Word2Vec(data_tok, \n",
    "                 vector_size=32,      # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5,     # define context as a 5-word window around the target word\n",
    "                 workers=4).wv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.2350323 ,  3.9130793 , -0.04143815, -6.911165  , -0.89529926,\n",
       "        0.30524158, -3.6381266 ,  3.0657196 ,  0.44838217, -2.7834735 ,\n",
       "        2.9404259 , -0.06854331,  2.2036445 ,  1.4172473 , -0.544135  ,\n",
       "        2.549206  , -0.59429175,  2.3207686 , -0.32980213,  2.7078602 ,\n",
       "       -3.4360175 ,  3.3906186 , -3.799569  ,  2.9482298 ,  1.5113999 ,\n",
       "       -2.9682817 , -1.0350368 , -1.572801  , -2.4871223 , -5.853968  ,\n",
       "       -0.7093816 ,  1.0439152 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can get word vectors !\n",
    "\n",
    "model.get_vector('sipariş')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hijyen', 0.7837107181549072),\n",
       " ('devlet', 0.7690507173538208),\n",
       " ('maske', 0.7676494121551514),\n",
       " ('görev', 0.7640803456306458),\n",
       " ('çalışanların', 0.7287359237670898),\n",
       " ('müşteriler', 0.7226987481117249),\n",
       " ('çalışanıyım', 0.715496301651001),\n",
       " ('tavrı', 0.7132790088653564),\n",
       " ('yaşanan', 0.7126019597053528),\n",
       " ('aksaklıklara', 0.7107350826263428)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or query similar words directly. Go play with it!\n",
    "model.most_similar('sağlık')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kontrol', 0.6618170738220215),\n",
       " ('iptal', 0.6576076149940491),\n",
       " ('kabul', 0.6235346794128418),\n",
       " ('teslim', 0.5533836483955383),\n",
       " ('devam', 0.5488733649253845),\n",
       " ('nefret', 0.54137122631073),\n",
       " ('i̇ade', 0.5411494970321655),\n",
       " ('takip', 0.5397305488586426),\n",
       " ('fark', 0.5317937135696411),\n",
       " ('talep', 0.5311428308486938)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"iade\", \"sipariş\"], negative=[\"kırık\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = sorted(model.vocab.keys(),  key=lambda word: model.vocab[word].count, reverse=True)[:1000]\n",
    "#print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'halde', 'iki', 'geldik', 'yaptığım', 'buna', 'seferinde', 'sürede', 'edilecek', 'sizinle']\n"
     ]
    }
   ],
   "source": [
    "words = sorted(model.key_to_index .keys(), \n",
    "               key=lambda word: model.get_vecattr(word, \"count\"),\n",
    "               reverse=True)[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, compute it's vector with model\n",
    "word_vectors = np.array([model[w] for w in words]) # MY CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cb25682cf69a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), 100)\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear projection: PCA\n",
    "\n",
    "The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
    "\n",
    "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n",
    "\n",
    "\n",
    "Under the hood, it attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
    "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
    "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
    "word_vectors_pca = # YOUR CODE\n",
    "\n",
    "\n",
    "# and maybe MORE OF YOUR CODE here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
    "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
    "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's draw it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
    "\n",
    "# hover a mouse over there and see if you can identify the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing neighbors with t-SNE\n",
    "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
    "\n",
    "If we instead want to focus on keeping neighboring points near, we could use TSNE, which is itself an embedding method. Here you can read __[more on TSNE](https://distill.pub/2016/misread-tsne/)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# map word vectors onto 2d plane with TSNE. hint: don't panic it may take a minute or two to fit.\n",
    "# normalize them as just lke with pca\n",
    "\n",
    "\n",
    "word_tsne = #YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing phrases\n",
    "\n",
    "Word embeddings can also be used to represent short phrases. The simplest way is to take __an average__ of vectors for all tokens in the phrase with some weights.\n",
    "\n",
    "This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.\n",
    "\n",
    "Let's try this new hammer on our data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase):\n",
    "    \"\"\"\n",
    "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
    "    \"\"\"\n",
    "    # 1. lowercase phrase\n",
    "    # 2. tokenize phrase\n",
    "    # 3. average word vectors for all words in tokenized phrase\n",
    "    # skip words that are not in model's vocabulary\n",
    "    # if all words are missing from vocabulary, return zeros\n",
    "    \n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    \n",
    "    # YOUR CODE\n",
    "    \n",
    "    return vector\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n",
    "\n",
    "assert np.allclose(vector[::10],\n",
    "                   np.array([ 0.31807372, -0.02558171,  0.0933293 , -0.1002182 , -1.0278689 ,\n",
    "                             -0.16621883,  0.05083408,  0.17989802,  1.3701859 ,  0.08655966],\n",
    "                              dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's only consider ~5k phrases for a first run.\n",
    "chosen_phrases = data[::len(data) // 1000]\n",
    "\n",
    "# compute vectors for chosen phrases\n",
    "phrase_vectors = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
    "assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map vectors into 2d space with pca, tsne or your other method of choice\n",
    "# don't forget to normalize\n",
    "\n",
    "phrase_vectors_2d = TSNE().fit_transform(phrase_vectors)\n",
    "\n",
    "phrase_vectors_2d = (phrase_vectors_2d - phrase_vectors_2d.mean(axis=0)) / phrase_vectors_2d.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(phrase_vectors_2d[:, 0], phrase_vectors_2d[:, 1],\n",
    "             phrase=[phrase[:50] for phrase in chosen_phrases],\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute vector embedding for all lines in data\n",
    "data_vectors = np.array([get_phrase_embedding(l) for l in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(query, k=10):\n",
    "    \"\"\"\n",
    "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
    "    similarity should be measured as cosine between query and line embedding vectors\n",
    "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "    \n",
    "    return <YOUR CODE: top-k lines starting from most similar>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
    "\n",
    "print(''.join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[0] == 'How do I get to the dark web?\\n'\n",
    "assert results[3] == 'What can I do to save the world?\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(query=\"How does Trump?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(query=\"Why don't i ask a question myself?\", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Now what?__\n",
    "* Try running TSNE on all data, not just 1000 phrases\n",
    "* See what other embeddings are there in the model zoo: `gensim.downloader.info()`\n",
    "* Take a look at [FastText](https://github.com/facebookresearch/fastText) embeddings\n",
    "* Optimize find_nearest with locality-sensitive hashing: use [nearpy](https://github.com/pixelogik/NearPy) or `sklearn.neighbors`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
